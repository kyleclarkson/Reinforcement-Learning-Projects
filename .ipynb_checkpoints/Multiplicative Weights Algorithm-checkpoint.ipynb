{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online Decision Making - The Multiplicative Weights Algorithm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An <b><i>online problem</i></b> is a problem where the input arrives in a piecewise fashion. \n",
    "This is in contrast to typical <b><i>offline problems</i></b> in which the input is available in its entirety.\n",
    "An <b><i>online algorithm (strategy) </i></b> is a means to solve an online algorithm. \n",
    "Each time a new piece of the input is given to the algorithm, a irrevocable <b><i>decision/action</i></b> can be preformed by the algorithm.\n",
    "\n",
    "Many offline problems have an online equivalent. Consider the scheduling of tasks to various machines to minimize the makespan (the longest time any machine will run.) \n",
    "Here, instead of all jobs (and the ammount of time needed to finish each job) beinging made available at the start of the assignment process, jobs arrive at in an online fashion.\n",
    "Other classic online problems are the ski rental problem and cow path's problem.\n",
    "<a href=\"https://en.wikipedia.org/wiki/Ski_rental_problem\">ski rental</a> and \n",
    "<a href=\"https://en.wikipedia.org/wiki/Linear_search_problem\">cow paths</a> problems.\n",
    "\n",
    "\n"
   ]
  },
  {
<<<<<<< Updated upstream
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
=======
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model\n",
    "\n",
    "At each step over a time horizion of $T \\geq 1$, exactly one action from a set $A$ of $n \\geq 2$ actions is taken. \n",
    "For each time step $t=1,2, \\ldots T:$\n",
    "<ul>\n",
    "<li>The algorithm chooses a probability distribution $p^t$ over $A$.</li>\n",
    "<li>An adversary picks a reward vector $r^t \\colon A \\rightarrow [-1,1]$</li>\n",
    "<li>An action $a^t$ is chosen according to the distribution $p^t$ which yields reward $r^t(a^t)$.</li>\n",
    "<li>The algorithm learns the entire reward vector $r^t$.</li>\n",
    "</ul>\n",
    "\n",
    "That is, $p^t$ is a function of the reward vectors $r^1, \\ldots, r^{t-1}$ and taken actions $a^1, \\ldots, a^{t-1}$. Meanwhile the adversary chooses $r^t$ given the knowledge of $p^1, \\ldots, p^t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a good online strategy?\n",
    "\n",
    "We would like to determine an algorithm that will maximize the (expected) reward over the $T$ timesteps. However given the model, there are instances where the advesary can ensure the action with largest probability (e.g. at least $\\frac{1}{2}$) has small reward (i.e. $-1$.) This results in total expected reward to be nonpositive yet the best sequence of actions can yield reward at most $T$.\n",
    "\n",
    "Instead of comparing the expected reward of an algorithm to that of the best action sequence over the $T$ steps, we will compare it to the reward resulting from the best ***fixed action*** in hindsight. This leads to the definition of regret:\n",
    "\n",
    "***Regret***: For fixed reward vector $r^1, \\ldots, r^T$, the **regret** of the action sequence $a^1, \\ldots, a^T$ is:\n",
    "\\begin{align*}\n",
    "R_T = \\max\\limits_{a \\in A} \\sum_{t=1}^{T}r^t(a) - \\sum_{t=1}^{T}r^t(a^t)\n",
    "\\end{align*}\n",
    "\n",
    "where the first term is for a fixed action $a \\in A$ and the latter term is result of our algorithm.\n",
    "Instead of maximizing expected reward, our aim is to ***minimize expected regret***.\n",
    "\n",
    "As rewards are scaled to be within $[-1,1]$, an upper bound on total regret is $2T$. We would like to aim for an algorithm that achieves regret which is at most linear in $T$ (i.e. is $o(T)$.) \n",
    "One algorithm is known as **follow-the-leader** where at each timestep $t$ action $a$ with maximum cumulative reward $\\sum_{u=1}^{t-1}r^{u}(a)$ is chosen.\n",
    "However there are instances where regret grows at least linearly in $T$.\n",
    "This is common to any deterministic algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
>>>>>>> Stashed changes
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
